{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c88f8d-fb6b-48ab-a5f7-257634eccae0",
   "metadata": {},
   "source": [
    "# **Part IV: Modelling**\n",
    "\n",
    "In this notebook, we would build several classification models and compare them\n",
    "\n",
    "## 1. Preparations\n",
    "- Load Modules and Datasets\n",
    "- Preprocessing\n",
    "\n",
    "I used to decide to integrate preprocessing part into the pipeline. But now I changed my mind. Because we need to drop those unrelevant features out of original data (maybe integrate it also in the pipeline?)\n",
    "\n",
    "## 2. Modeling\n",
    "- Model build\n",
    "  - Tree Model\n",
    "    - Decision Tree\n",
    "  - Linear Model\n",
    "    - Logistic Regression\n",
    "  - Advanced Models with advanced preprocessing techniques\n",
    "  - If the model is complex (maybe self-designed), we might need to write models in different files and import them\n",
    "- Model comparison\n",
    "  - method\n",
    "    - cross validation (K-fold)\n",
    "      - train/validation split\n",
    "        - for biased situation\n",
    "    - bootstrap\n",
    "  - metrics (for classification problem)\n",
    "    - accuracy\n",
    "    - precision / recall\n",
    "    - f1-score\n",
    "    - auc\n",
    "    - confusion matrix\n",
    "- Model validation\n",
    "  - Sanity Check\n",
    "  - Sensitive check ...\n",
    "\n",
    "## 3. Prediction\n",
    "- generate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df522c88-8a6d-41e6-bfe8-4c4f800a61dd",
   "metadata": {},
   "source": [
    "# 1. Preparations\n",
    "\n",
    "We start by loading necessary **Modules** and maybe google colab. Then we import the Titanic **Dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8ff6b78-738b-4a3c-8145-17557e2f98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from src.features import FeaturePreprocessor_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53e28262-f9cf-496b-83e9-c3ce7395bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "\n",
    "train = pd.read_csv(\"../data/raw/train.csv\")\n",
    "train_copy = train.copy()\n",
    "test = pd.read_csv(\"../data/raw/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4c2db8-50d9-4247-8082-febc6f9bdc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics for classification problem\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb13ebe-acc6-412b-be80-21585978f283",
   "metadata": {},
   "source": [
    "# 2. Modeling\n",
    "\n",
    "Considerations are that different models would require different make_feature funcs, it is not a one2one question but many2many question. I belive the most confusing part lies in the feature selection. The feature selection would be only complete after we got ``print(train_processed.columns)``, which couldn't be finished inside the make_features only. Idea is that we could write a feature_selection func inside that to integrate a complete pipeline.\n",
    "\n",
    "## 2.1 Data Preprocessing version I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c37f034-8e4e-46ee-a5d3-6fde692cdd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket',\n",
      "       'Fare', 'Cabin', 'Cabin_missing_indicator', 'Sex_male', 'Pclass_2',\n",
      "       'Pclass_3', 'Embarked_Q', 'Embarked_S', 'Embarked_nan', 'Fare_log'],\n",
      "      dtype='object')\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# make_features could produce many features but only important ones would be kept \n",
    "\n",
    "preprocessor_v1 = FeaturePreprocessor_v1()\n",
    "train_processed = preprocessor_v1.fit_transform(train)\n",
    "print(train_processed.columns)\n",
    "\n",
    "# I was thinking, maybe after this I could go back to complete features.py (but what if same preprocessor used for )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a197e7-d748-4d54-8add-7ce1a803b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "Features_selected_v1 = ['Age', 'Fare', 'Cabin_missing_indicator', 'Sex_male', 'Pclass_2', 'Pclass_3', 'Embarked_Q', 'Embarked_S']\n",
    "X = train_processed[Features_selected_v1]\n",
    "y = train_processed['Survived']\n",
    "\n",
    "# train & validation split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f4f4a-aa21-4cdf-b415-e99f0305e968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de81081f-cea7-4142-9b0c-9224c9aa0ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12de8954-88e7-4bac-a592-08bba2423f62",
   "metadata": {},
   "source": [
    "# 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ab079fc-ad62-431f-a106-2ebc201393ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.776536312849162\n",
      "[[95 15]\n",
      " [25 44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.83       110\n",
      "           1       0.75      0.64      0.69        69\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.77      0.75      0.76       179\n",
      "weighted avg       0.77      0.78      0.77       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b58fe-0fc3-4b43-886d-2b558db89f50",
   "metadata": {},
   "source": [
    "# 3. Baseline Model\n",
    "\n",
    "A baseline model is selected after "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
